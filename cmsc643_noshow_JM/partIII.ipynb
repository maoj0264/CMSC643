{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import noshow_lib.util as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw_data_path': 'data',\n",
       " 'raw_data_csv': 'KaggleV2-May-2016.csv',\n",
       " 'processed_data_path': 'processed_data',\n",
       " 'train_csv': 'train_set.csv',\n",
       " 'test_csv': 'test_set.csv',\n",
       " 'objstore_path': 'objects',\n",
       " 'feature_pipeline_file': 'feature_pipeline.pkl',\n",
       " 'labels_pipeline_file': 'labels_pipeline.pkl'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.file_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config = utils.file_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import noshow_lib.preprocess as preprocess\n",
    "train_X, train_y = preprocess.load_train_data(config=file_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90526, 113)\n",
      "(90526,)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X, test_y = preprocess.load_test_data(config=file_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 113)\n",
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "print(test_X.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Ensembles and Final Result\n",
    "\n",
    "## AdaBoost\n",
    "\n",
    "Train an AdaBoost classifier and compare its performance to results obtained in Part II using 10 fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "SEED = 7\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7973068170408475\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost code goes here\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "num_trees = 70\n",
    "\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=SEED)\n",
    "results = cross_val_score(model, train_X, train_y, cv=10)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgBoost\n",
    "\n",
    "Train an xgBoost classifier and compare its performance to results in Part II using 10 fold CV. `sklearn` has a gradient boosting model included http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html which you can use. The `xgboost` package https://xgboost.readthedocs.io/en/latest/python/python_intro.htmlhas a wrapper you can use with sklearn as well https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn. The latter is more efficient at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7981574345977211\n"
     ]
    }
   ],
   "source": [
    "# xgboost code here\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "num_trees = 100\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=SEED)\n",
    "results = cross_val_score(model, train_X, train_y, cv=10)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "Choose a set of 5 or so classifiers. Write a function that trains an ensemble using stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier,ExtraTreesClassifier)\n",
    "\n",
    "def get_models():\n",
    "    #Generate a library of base learners\n",
    "    rfc = RandomForestClassifier(max_features= 'sqrt', n_jobs=4, n_estimators=150, oob_score = True) \n",
    "    gb = GradientBoostingClassifier(n_estimators=100, random_state=SEED)\n",
    "    rf = RandomForestClassifier(n_estimators=150, n_jobs=4, max_features=20, random_state=SEED)\n",
    "    ab = AdaBoostClassifier(n_estimators=70, random_state=SEED)\n",
    "    et = ExtraTreesClassifier(n_estimators=10, criterion='entropy')\n",
    "\n",
    "    models = {'random forest-sqrt': rfc,\n",
    "              'GradientBoosting': gb,\n",
    "              'random forest-20': rf,\n",
    "              'AdaBoost': ab,\n",
    "              'ExtraTreesClassifier': et,\n",
    "              }\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_base_learners(base_learners, X, y):\n",
    "    for i, (name, m) in enumerate(base_learners.items()):\n",
    "        m.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_base_learners(pred_base_learners, X):\n",
    "    P = np.zeros((X.shape[0], len(pred_base_learners)))\n",
    "\n",
    "    for i, (name, m) in enumerate(pred_base_learners.items()):\n",
    "        p = m.predict_proba(X)\n",
    "        P[:, i] = p[:, 1]\n",
    "\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "lr = LogisticRegression(random_state=SEED)\n",
    "\n",
    "def build_stack_ensemble(X, y):\n",
    "    \n",
    "    base_learners = get_models()\n",
    "    lr_learner = clone(lr)    \n",
    "    \n",
    "    train_base_learners(base_learners, X, y)\n",
    "\n",
    "    print(\"Generating cross-validated predictions...\")\n",
    "    cv_preds, cv_y = [], []\n",
    "    \n",
    "    generator = StratifiedShuffleSplit(n_splits=1)\n",
    "    for train_index, test_index in generator.split(X, y):\n",
    "        fold_xtrain, fold_xtest = X[train_index], X[test_index]\n",
    "        fold_ytrain, fold_ytest = y[train_index], y[test_index]\n",
    "\n",
    "        fold_base_learners = {name: clone(model)\n",
    "                              for name, model in base_learners.items()}\n",
    "        \n",
    "        train_base_learners(fold_base_learners, fold_xtrain, fold_ytrain)\n",
    "\n",
    "        fold_P_base = predict_base_learners(fold_base_learners, fold_xtest)\n",
    "\n",
    "        cv_preds.append(fold_P_base)\n",
    "        cv_y.append(fold_ytest)\n",
    "\n",
    "    cv_preds = np.vstack(cv_preds)\n",
    "    cv_y = np.hstack(cv_y)\n",
    "\n",
    "    # Train meta learner\n",
    "    lr_learner.fit(cv_preds, cv_y)\n",
    "    \n",
    "    print(\"Done\")\n",
    "\n",
    "    return base_learners, lr_learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 10-fold cross validation to measure performance of your stacked classifier. See Part II solution to see how to roll your own sklearn classifier along with http://scikit-learn.org/stable/developers/contributing.html#rolling-your-own-estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Result\n",
    "\n",
    "Choose a single model based on all previous project steps. Train this model on the complete training dataset and measure it's performance on the held out test set.\n",
    "\n",
    "Compare to the 10-fold CV estimate you got previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(base_learners, meta_learner, inpX):\n",
    "    P_pred = predict_base_learners(base_learners, inpX)\n",
    "    return P_pred, meta_learner.predict_proba(P_pred)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating cross-validated predictions...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# final result goes here\n",
    "# Train with stacking\n",
    "cv_base_learners, cv_meta_learner = build_stack_ensemble(train_X, train_y)\n",
    "\n",
    "P_pred, p = ensemble_predict(cv_base_learners, cv_meta_learner, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensemble ROC-AUC score: 0.749\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "print(\"\\nEnsemble ROC-AUC score: %.3f\" % roc_auc_score(test_y, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensemble mean AUC score: 0.795\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "results = cross_val_score(cv_meta_learner, train_X, train_y, cv=10)\n",
    "print(\"\\nEnsemble mean AUC score: %.3f\" % results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "My stacking doesn't have a better performance than AdaBoost and GradientBoosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StackingClassifier Accuracy: 0.78 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "### Try with python library\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "base_learners = get_models()\n",
    "\n",
    "lr = LogisticRegression(random_state=SEED)\n",
    "\n",
    "sclf = StackingClassifier(classifiers=list(base_learners.values()), meta_classifier=lr)\n",
    "\n",
    "scores = cross_val_score(sclf, train_X, train_y, cv=10, scoring='accuracy')\n",
    "print(\"StackingClassifier Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Python stacking library performance is even worse than mine. Why??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
